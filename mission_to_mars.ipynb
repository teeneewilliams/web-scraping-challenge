{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import splinter and set path to the chromedriver\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrape data for Mars News\n",
    "def scrape_news():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    \n",
    "    browser.visit(url)\n",
    "\n",
    "    # To avoid lag time\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # To scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # To get the latest news title and paragraph text\n",
    "    article = soup.find('div', class_='list_text')\n",
    "    news_title = article.find('a').text\n",
    "    news_p = article.find('div', class_='article_teaser_body').text\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    return {'title' : news_title, 'text' : news_p}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrape images \n",
    "\n",
    "def scrape_JPL_image():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # To visit the url to grap the jpl space images\n",
    "    nasa_url = \"https://www.jpl.nasa.gov\"\n",
    "    jpl_query = \"/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(nasa_url+jpl_query)\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # To scrape the page into the soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # To find the image url for the current featured mars image\n",
    "    article = soup.find('div', class_='carousel_items').find('article')\n",
    "    featured_image_url = article['style'].split(\"'\")[1]\n",
    "    \n",
    "    browser.quit()\n",
    "\n",
    "    return nasa_url+featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the facts about mars and scrape into a table\n",
    "\n",
    "def scrape_mars_facts():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # To visit the url to scrape the images\n",
    "    url = \"https://space-facts.com/mars/\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # To read html using pandas\n",
    "    html = browser.html\n",
    "    tables = pd.read_html(html)\n",
    "    \n",
    "    # To scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "    df = pd.DataFrame(tables[0])\n",
    "    table_html = df.to_html(index=False, border=1, header=False,\n",
    "                            classes=[\"table\", \"table-responsive\", \"table-striped\"], \n",
    "                            justify='center')\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    return table_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrape data about Mars Hemispheres\n",
    "\n",
    "def scrape_mars_hemispheres():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # To visit the USGS site \n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # To scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # To find all items\n",
    "    items = soup.find_all('div', class_='item')\n",
    "    \n",
    "    # To initialize a list\n",
    "    hemisphere_image_urls = []\n",
    "    \n",
    "    # To find the titles and image urls for the Hemispheres\n",
    "    usgs_url = \"https://astrogeology.usgs.gov\"\n",
    "    for item in items:\n",
    "        \n",
    "        # To find title of this item\n",
    "        title = item.find('h3').text\n",
    "        \n",
    "        # To initialize a dictionary\n",
    "        hemisphere = {}\n",
    "        \n",
    "        # To find the url where this item is explained in detail\n",
    "        item_url = item.find('a')['href']\n",
    "        \n",
    "        # To scrape the item's url to find image link and title\n",
    "        # To scrape by clicking the link for each item \n",
    "        browser.find_by_text(title).click()\n",
    "        soup2 = bs(browser.html, \"html.parser\")\n",
    "        \n",
    "        # To find the link for the full size image \n",
    "        # NOTE: the first link is to get the jpg\n",
    "        # NOTE2: the second link is to get the tif(full size)\n",
    "        imgs = soup2.find('div', class_=\"downloads\").find_all('a')\n",
    "        jpg_url = imgs[0]['href']\n",
    "        \n",
    "        # To add the img url to the dictionary\n",
    "        hemisphere['title'] = title\n",
    "        hemisphere['img_url'] = jpg_url\n",
    "        \n",
    "        # To append the dictionary to the list\n",
    "        hemisphere_image_urls.append(hemisphere)\n",
    "        \n",
    "        # To go back to the USGS Astrogeology site\n",
    "        browser.back()\n",
    "        \n",
    "        # To quit the browser\n",
    "    browser.quit()\n",
    "\n",
    "    return hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To scrape data for mars hemispheres\n",
    "scrape_mars_hemispheres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
